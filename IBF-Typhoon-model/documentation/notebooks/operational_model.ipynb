{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    "    KFold)\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    mean_squared_error,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer)\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    " \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    SequentialFeatureSelector,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif)\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import openpyxl\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "wor_dir=\"/home/fbf/\"\n",
    "\n",
    "\n",
    "wor_dir=\"C:/Typhoon_IBF/Typhoon-Impact-based-forecasting-model/IBF-Typhoon-model\"\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input_data=pd.read_csv(\"data/model_input/df_modelinput_july.csv\") \n",
    "tphoon_events=combined_input_data[['typhoon','DAM_perc_dmg']].groupby('typhoon').size().to_dict()\n",
    "### for hyper parameter optimization we looped over typhoon events with at least 100 data entries  \n",
    "typhoons_with_impact_data= [key for key,value in tphoon_events.items() if value>100]\n",
    "#combined_input_data=combined_input_data[combined_input_data.typhoon.isin(typhoons_with_impact_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_zeros(x):\n",
    "    x_max = 25\n",
    "    y_max = 50\n",
    "    \n",
    "    v_max = x[0]\n",
    "    rainfall_max = x[1]\n",
    "    damage = x[2]\n",
    "    if pd.notnull(damage):\n",
    "        value = damage\n",
    "    elif v_max > x_max or rainfall_max > y_max:\n",
    "        value =damage\n",
    "    elif (v_max < np.sqrt((1- (rainfall_max**2/y_max ** 2))*x_max ** 2)):\n",
    "        value = 0\n",
    "    #elif ((v_max < x_max)  and  (rainfall_max_6h < y_max) ):\n",
    "    #elif (v_max < x_max ):\n",
    "    #value = 0\n",
    "    else:\n",
    "        value = np.nan\n",
    "\n",
    "    return value\n",
    "\n",
    "combined_input_data[\"DAM_perc_dmg\"] = combined_input_data[[\"HAZ_v_max\", \"HAZ_rainfall_Total\", \"DAM_perc_dmg\"]].apply(set_zeros, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result of parameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_params_xgb_regr={'colsample_bytree': 1.0,\n",
    "                          'eta': 0.05,\n",
    "                          'gamma': 1.0,\n",
    "                          'learning_rate': 0.025,\n",
    "                          'max_depth': 8,\n",
    "                          'min_child_weight': 1.0,\n",
    "                          'n_estimators': 80.0,\n",
    "                          'early_stopping_rounds ':10,\n",
    "                          'subsample': 0.8\n",
    "                          }\n",
    "wor_dir=\"C:/Typhoon_IBF/Typhoon-Impact-based-forecasting-model/IBF-Typhoon-model\"\n",
    "file_name = \"models/output/v2/selected_params_xgb_regr.p\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "\n",
    "SEED = 314159265\n",
    " \n",
    "test_size = 0.1\n",
    "\n",
    "# Full dataset for feature selection\n",
    "\n",
    "combined_input_data_ = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    " \n",
    "\n",
    "selected_features_xgb_regr= ['HAZ_rainfall_max_24h','HAZ_v_max',#'HAZ_dis_track_min',\n",
    "                             'TOP_mean_slope',\n",
    "                                       'TOP_mean_elevation_m','TOP_ruggedness_stdev','TOP_mean_ruggedness','TOP_slope_stdev',\n",
    "                                       'VUL_poverty_perc','GEN_with_coast','VUL_Housing_Units','VUL_StrongRoof_StrongWall',\n",
    "                                       'VUL_StrongRoof_LightWall','VUL_StrongRoof_SalvageWall','VUL_LightRoof_StrongWall',\n",
    "                                       'VUL_LightRoof_LightWall','VUL_SalvagedRoof_StrongWall','VUL_SalvagedRoof_LightWall',\n",
    "                                       'VUL_SalvagedRoof_SalvageWall','VUL_vulnerable_groups','VUL_pantawid_pamilya_beneficiary']\n",
    "\n",
    "selected_features_xgb_regr_all_event= ['HAZ_v_max',#'HAZ_dis_track_min',\n",
    "                                       'TOP_mean_slope',\n",
    "                                       'TOP_mean_elevation_m','TOP_ruggedness_stdev','TOP_mean_ruggedness','TOP_slope_stdev',\n",
    "                                       'VUL_poverty_perc','GEN_with_coast','VUL_Housing_Units','VUL_StrongRoof_StrongWall',\n",
    "                                       'VUL_StrongRoof_LightWall','VUL_StrongRoof_SalvageWall','VUL_LightRoof_StrongWall',\n",
    "                                       'VUL_LightRoof_LightWall','VUL_SalvagedRoof_StrongWall','VUL_SalvagedRoof_LightWall',\n",
    "                                       'VUL_SalvagedRoof_SalvageWall','VUL_vulnerable_groups','VUL_pantawid_pamilya_beneficiary']\n",
    "\n",
    "X = combined_input_data_[selected_features_xgb_regr]\n",
    "\n",
    "y = combined_input_data_[\"DAM_perc_dmg\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "# fit model no training data\n",
    " \n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "#selected_features_xgb_regr_all_event\n",
    "reg = xgb.XGBRegressor(base_score=0.5,\n",
    "             booster='gbtree', \n",
    "             subsample=0.8,\n",
    "             eta=0.05,\n",
    "             max_depth=8, \n",
    "             colsample_bylevel=1,\n",
    "             colsample_bynode=1, \n",
    "             colsample_bytree=1,\n",
    "             early_stopping_rounds=10, \n",
    "             eval_metric=mean_absolute_error,\n",
    "             gamma=1, \n",
    "             objective='reg:squarederror',\n",
    "             gpu_id=-1, \n",
    "             grow_policy='depthwise', \n",
    "             learning_rate=0.025,\n",
    "             min_child_weight=1,\n",
    "             n_estimators=200,        \n",
    "             random_state=42,\n",
    "             tree_method=\"hist\",\n",
    "             )\n",
    "\n",
    "\n",
    "eval_set=[(X_train, y_train), ( X_test, y_test)]\n",
    "reg.fit(X, y, eval_set=eval_set)\n",
    " \n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X)\n",
    "y_pred[y_pred < 0] = 0\n",
    "combined_input_data_['DMG_predicted']=y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated impact, number of completely damaged houses per typhoon event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def division(x, y):\n",
    "    try:\n",
    "        value = int((x * y)/100)\n",
    "        \n",
    "    except:\n",
    "        value = np.nan\n",
    "    \n",
    "    return x if value>x else value  \n",
    "\n",
    "f_path = os.path.join(wor_dir,\"data/all_predisaster_indicators.csv\")\n",
    " \n",
    "df_predisasters = pd.read_csv(f_path)\n",
    "\n",
    "df_ = pd.merge(combined_input_data_[['Mun_Code','typhoon','DAM_perc_dmg','DMG_predicted']], df_predisasters[['Housing Units','Mun_Code']],  how='left', left_on='Mun_Code', right_on = 'Mun_Code') \n",
    "\n",
    "df_[\"number_dmg_Build_prediction\"] = df_.apply(lambda x: division(x[\"Housing Units\"], x[\"DMG_predicted\"]), axis=1).values\n",
    "df_[\"number_dmg_Build_dromic\"] = df_.apply(lambda x: division(x[\"Housing Units\"], x[\"DAM_perc_dmg\"]), axis=1).values\n",
    "\n",
    "df_im_haz=df_.groupby('typhoon').agg(NUmber_of_affected_municipality=('Mun_Code','count'),\n",
    "                                     average_ML_Model=('DMG_predicted', average),\n",
    "                                     average_dromic=('DAM_perc_dmg', average),\n",
    "                                     Total_buildings_ML_Model=('number_dmg_Build_prediction', sum),\n",
    "                                     Total_buildings_dromic=('number_dmg_Build_dromic', sum)).sort_values(by='Total_buildings_dromic',ascending=False).reset_index()\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c56dc72bed4e123c21144b1399355bbd07b8d587f6360d6b6ea22ee2ab335a35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
