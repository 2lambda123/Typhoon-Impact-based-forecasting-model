{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a1514",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model for the binary classfication model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    "\n",
    "### Binary Classification\n",
    "At the end of this section we will obtain  the optimal Binary Classification models and the performance estimates, \n",
    "for a 10% threshold. Two models are implemented: Random Forest Classifier, XGBoost Classifier. \n",
    "First, the model is trained on the full dataset to obtain the optimal features followed by a model \n",
    "that obtains the performance estimate using Nested Cross Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import openpyxl\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac563676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_damage_class(x):\n",
    "    damage = x[0]   \n",
    "    if damage > 0.1:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    "    typhoon=typhoons_with_impact_data\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "\n",
    "        df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "        df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-typhoon-model\"\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c601cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from models.binary_classification.rf_binary import (rf_binary_features,rf_binary_performance,)\n",
    "from models.binary_classification.xgb_binary import (xgb_binary_features,xgb_binary_performance,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe30206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_input_data=pd.read_csv(\"data\\\\combined_input_data.csv\")\n",
    "combined_input_data[\"binary_dmg\"] = combined_input_data[[\"perc_dmg\"]].apply(binary_damage_class, axis=\"columns\")\n",
    "\n",
    "typhoons_with_impact_data=['bopha2012', 'conson2010', 'durian2006', 'fengshen2008',\n",
    "       'fung-wong2014', 'goni2015', 'goni2020', 'hagupit2014','haima2016', 'haiyan2013', 'kalmaegi2014', 'kammuri2019',\n",
    "       'ketsana2009', 'koppu2015', 'krosa2013', 'lingling2014','mangkhut2018', 'mekkhala2015', 'melor2015', 'mujigae2015',\n",
    "       'nari2013', 'nesat2011', 'nock-ten2016', 'noul2015','rammasun2014', 'sarika2016', 'trami2013', 'usagi2013', 'utor2013',\n",
    "       'vamco2020']\n",
    "\n",
    "combined_input_data=combined_input_data[combined_input_data.typhoon.isin(typhoons_with_impact_data)]\n",
    "\n",
    "\n",
    "combined_input_data.rename(columns ={\"rainfall_Total\":\"HAZ_rainfall_Total\",\n",
    "                                     'rainfall_max_6h':'HAZ_rainfall_max_6h',\n",
    "                                     'rainfall_max_24h':'HAZ_rainfall_max_24h',\n",
    "                                     'v_max':'HAZ_v_max',\n",
    "                                     'dis_track_min':'HAZ_dis_track_min',\n",
    "                                     'binary_dmg':'DAM_binary_dmg',\n",
    "                                     'perc_dmg':'DAM_perc_dmg',\n",
    "                                    'landslide_per':'GEN_landslide_per',\n",
    "                                    'stormsurge_per':'GEN_stormsurge_per',\n",
    "                                    'Bu_p_inSSA':'GEN_Bu_p_inSSA',\n",
    "                                    'Bu_p_LS':'GEN_Bu_p_LS',\n",
    "                                     'Red_per_LSbldg':'GEN_Red_per_LSbldg',\n",
    "                                    'Or_per_LSblg':'GEN_Or_per_LSblg',\n",
    "                                     'Yel_per_LSSAb':'GEN_Yel_per_LSSAb',\n",
    "                                    'RED_per_SSAbldg':'GEN_RED_per_SSAbldg',\n",
    "                                     'OR_per_SSAbldg':'GEN_OR_per_SSAbldg',\n",
    "                                    'Yellow_per_LSbl':'GEN_Yellow_per_LSbl',\n",
    "                                     'mean_slope':'TOP_mean_slope',\n",
    "                                    'mean_elevation_m':'TOP_mean_elevation_m',\n",
    "                                     'ruggedness_stdev':'TOP_ruggedness_stdev',\n",
    "                                    'mean_ruggedness':'TOP_mean_ruggedness',\n",
    "                                     'slope_stdev':'TOP_slope_stdev',\n",
    "                                     'poverty_perc':'VUL_poverty_perc',\n",
    "                                    'with_coast':'GEN_with_coast',\n",
    "                                     'coast_length':'GEN_coast_length',\n",
    "                                     'Housing Units':'VUL_Housing_Units',\n",
    "                                    'Strong Roof/Strong Wall':\"VUL_StrongRoof_StrongWall\",\n",
    "                                    'Strong Roof/Light Wall':'VUL_StrongRoof_LightWall',\n",
    "                                    'Strong Roof/Salvage Wall':'VUL_StrongRoof_SalvageWall',\n",
    "                                    'Light Roof/Strong Wall':'VUL_LightRoof_StrongWall',\n",
    "                                    'Light Roof/Light Wall':'VUL_LightRoof_LightWall',\n",
    "                                    'Light Roof/Salvage Wall':'VUL_LightRoof_SalvageWall',\n",
    "                                    'Salvaged Roof/Strong Wall':'VUL_SalvagedRoof_StrongWall',\n",
    "                                    'Salvaged Roof/Light Wall':'VUL_SalvagedRoof_LightWall',\n",
    "                                    'Salvaged Roof/Salvage Wall':'VUL_SalvagedRoof_SalvageWall',\n",
    "                                    'vulnerable_groups':'VUL_vulnerable_groups',\n",
    "                                    'pantawid_pamilya_beneficiary':'VUL_pantawid_pamilya_beneficiary'},inplace=True)\n",
    "\n",
    "\n",
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary',\n",
    "        'DAM_binary_dmg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e1f45",
   "metadata": {},
   "source": [
    "####  Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 250],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [2, 8, 10, 15],\n",
    "        \"estimator__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_rf_binary, selected_params_rf_binary_full = rf_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(f\"Number of selected features RF Binary: {len(selected_features_rf_binary)}\")\n",
    "print(f\"Selected features RF Binary: {selected_features_rf_binary}\")\n",
    "print(f\"Selected Parameters RF Binary {selected_params_rf_binary_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578cc14",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "\n",
    " \n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [100, 250],\n",
    "        \"rf__max_depth\": [20, None],\n",
    "        \"rf__min_samples_split\": [2, 8, 15],\n",
    "        \"rf__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_rf_binary, selected_params_rf_binary = rf_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_rf_binary,\n",
    "    search_space=rf_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\selected_params_rf_binary.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_rf_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_rf_binary.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_rf_binary.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f88b8",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the XGBoost search grid for full dataset\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_xgb_binary, selected_params_xgb_binary_full = xgb_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    objective=\"binary:hinge\",\n",
    "    cv_splits=5,\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_n_iter=50,\n",
    "    GS_randomized=True,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332289",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)\n",
    "\n",
    "selected_features_xgb_binary=selected_features_xgb_regr\n",
    "\n",
    "# Setting the XGBoost search grid\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"xgb__gamma\": [0.1, 0.5, 2],\n",
    "        \"xgb__max_depth\": [6, 8],\n",
    "        \"xgb__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"xgb__n_estimators\": [100, 200],\n",
    "        \"xgb__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_xgb_binary, selected_params_xgb_binary = xgb_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_xgb_binary,\n",
    "    search_space=xgb_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    objective=\"binary:hinge\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\selected_params_xgb_binary.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_xgb_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_xgb_binary.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_xgb_binary.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb2ae7",
   "metadata": {},
   "source": [
    "#### Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "df_predicted_random = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = unweighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random = pd.concat([df_predicted_random, df_predicted_temp])\n",
    "\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_random.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random.to_csv(path, index=False)\n",
    "    \n",
    "df_predicted_random_weighted = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = weighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random_weighted = pd.concat(\n",
    "        [df_predicted_random_weighted, df_predicted_temp]\n",
    "    )\n",
    "\n",
    "    \n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_random_weighted.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random_weighted.to_csv(path, index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
