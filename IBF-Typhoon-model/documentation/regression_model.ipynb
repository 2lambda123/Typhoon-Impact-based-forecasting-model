{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3b3be3",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model.For the trigger model we used a Regression model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    " \n",
    "### Regression \n",
    "At the end of this section we will obtain  the optimal Regression models and the performance estimates. Two models are implemented: Random Forest, XGBoost.First, the model is trained on the full dataset to obtain the optimal features followed by a model that obtains the performance estimate using Nested Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "549be22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import openpyxl\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657df28e",
   "metadata": {},
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "795ea4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    "    typhoon=typhoons_with_impact_data\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "\n",
    "        df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "        df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n",
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "065cb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-typhoon-model\"\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07e349e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from models.regression.rf_regression import (rf_regression_features,rf_regression_performance,)\n",
    "from models.regression.xgb_regression import (xgb_regression_features,xgb_regression_performance,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad250b0",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16df7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input_data=pd.read_csv(\"data\\\\combined_input_data.csv\")\n",
    "\n",
    "typhoons_with_impact_data=['bopha2012', 'conson2010', 'durian2006', 'fengshen2008',\n",
    "       'fung-wong2014', 'goni2015', 'goni2020', 'hagupit2014','haima2016', 'haiyan2013', 'kalmaegi2014', 'kammuri2019',\n",
    "       'ketsana2009', 'koppu2015', 'krosa2013', 'lingling2014','mangkhut2018', 'mekkhala2015', 'melor2015', 'mujigae2015',\n",
    "       'nari2013', 'nesat2011', 'nock-ten2016', 'noul2015','rammasun2014', 'sarika2016', 'trami2013', 'usagi2013', 'utor2013',\n",
    "       'vamco2020']\n",
    "\n",
    "combined_input_data=combined_input_data[combined_input_data.typhoon.isin(typhoons_with_impact_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17b57ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_input_data['year']=combined_input_data['typhoon'].apply(lambda x: int(x[-4:]))\n",
    "\n",
    "combined_input_data.rename(columns ={\"rainfall_Total\":\"HAZ_rainfall_Total\",\n",
    "                                     'rainfall_max_6h':'HAZ_rainfall_max_6h',\n",
    "                                     'rainfall_max_24h':'HAZ_rainfall_max_24h',\n",
    "                                     'v_max':'HAZ_v_max',\n",
    "                                     'dis_track_min':'HAZ_dis_track_min',\n",
    "                                     'perc_dmg':'DAM_perc_dmg',\n",
    "                                    'landslide_per':'GEN_landslide_per',\n",
    "                                    'stormsurge_per':'GEN_stormsurge_per',\n",
    "                                    'Bu_p_inSSA':'GEN_Bu_p_inSSA',\n",
    "                                    'Bu_p_LS':'GEN_Bu_p_LS',\n",
    "                                     'Red_per_LSbldg':'GEN_Red_per_LSbldg',\n",
    "                                    'Or_per_LSblg':'GEN_Or_per_LSblg',\n",
    "                                     'Yel_per_LSSAb':'GEN_Yel_per_LSSAb',\n",
    "                                    'RED_per_SSAbldg':'GEN_RED_per_SSAbldg',\n",
    "                                     'OR_per_SSAbldg':'GEN_OR_per_SSAbldg',\n",
    "                                    'Yellow_per_LSbl':'GEN_Yellow_per_LSbl',\n",
    "                                     'mean_slope':'TOP_mean_slope',\n",
    "                                    'mean_elevation_m':'TOP_mean_elevation_m',\n",
    "                                     'ruggedness_stdev':'TOP_ruggedness_stdev',\n",
    "                                    'mean_ruggedness':'TOP_mean_ruggedness',\n",
    "                                     'slope_stdev':'TOP_slope_stdev',\n",
    "                                     'poverty_perc':'VUL_poverty_perc',\n",
    "                                    'with_coast':'GEN_with_coast',\n",
    "                                     'coast_length':'GEN_coast_length',\n",
    "                                     'Housing Units':'VUL_Housing_Units',\n",
    "                                    'Strong Roof/Strong Wall':\"VUL_StrongRoof_StrongWall\",\n",
    "                                    'Strong Roof/Light Wall':'VUL_StrongRoof_LightWall',\n",
    "                                    'Strong Roof/Salvage Wall':'VUL_StrongRoof_SalvageWall',\n",
    "                                    'Light Roof/Strong Wall':'VUL_LightRoof_StrongWall',\n",
    "                                    'Light Roof/Light Wall':'VUL_LightRoof_LightWall',\n",
    "                                    'Light Roof/Salvage Wall':'VUL_LightRoof_SalvageWall',\n",
    "                                    'Salvaged Roof/Strong Wall':'VUL_SalvagedRoof_StrongWall',\n",
    "                                    'Salvaged Roof/Light Wall':'VUL_SalvagedRoof_LightWall',\n",
    "                                    'Salvaged Roof/Salvage Wall':'VUL_SalvagedRoof_SalvageWall',\n",
    "                                    'vulnerable_groups':'VUL_vulnerable_groups',\n",
    "                                    'pantawid_pamilya_beneficiary':'VUL_pantawid_pamilya_beneficiary'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "103835c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary', \n",
    "        'DAM_perc_dmg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "567b12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451de586",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44ea121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset for feature selection\n",
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_perc_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c778aa",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Feature Selection is an important step in devloping a machine learning model.Data features used to train a machine learning model will influence model performance,less important features can have a negative impact on model performance.\n",
    "Feature Selection aims to solve the problem of identifying relevant features from a dataset by removing the less important features, which have little/no contribution to our target variable. Feature selection helps to achieve better model accuracy.\n",
    "\n",
    "There are different techniques for feature selection. For this research we used Recursive feature elimination (RFE),which is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model’s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.\n",
    "To find the optimal number of features we applied cross-validation with RFE on the entire data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b13869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting input varialbes\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 150],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [4, 5, 8],\n",
    "        \"estimator__min_samples_leaf\":[1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "(\n",
    "    selected_features_rf_regr,\n",
    "    selected_params_rf_regr_full,\n",
    ") = rf_regression_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    min_features_to_select=1,\n",
    "    cv_splits=3,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of selected features RF Regression {len(selected_features_rf_regr)}\"\n",
    ")\n",
    "print(f\"Selected features RF Regression: {selected_features_rf_regr}\")\n",
    "print(f\"Selected Parameters RF Regression: {selected_params_rf_regr_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783294a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on output previous cell\n",
    "selected_features_rf_regr=[\n",
    "    'HAZ_rainfall_Total', \n",
    "    'HAZ_rainfall_max_6h',\n",
    "    'HAZ_rainfall_max_24h',\n",
    "    'HAZ_v_max', \n",
    "    'HAZ_dis_track_min',\n",
    "    'GEN_landslide_per', \n",
    "    'GEN_stormsurge_per',\n",
    "    'GEN_Bu_p_inSSA', \n",
    "    'GEN_Bu_p_LS',\n",
    "    'GEN_Red_per_LSbldg',\n",
    "    'GEN_Yel_per_LSSAb',\n",
    "    'GEN_RED_per_SSAbldg', \n",
    "    'GEN_OR_per_SSAbldg',\n",
    "    'TOP_mean_slope', \n",
    "    'TOP_mean_elevation_m', \n",
    "    'TOP_ruggedness_stdev', \n",
    "    'TOP_mean_ruggedness', \n",
    "    'TOP_slope_stdev', \n",
    "    'VUL_poverty_perc', \n",
    "    'GEN_coast_length',\n",
    "    'VUL_Housing_Units', \n",
    "    'VUL_StrongRoof_StrongWall', \n",
    "    'VUL_StrongRoof_LightWall',\n",
    "    'VUL_StrongRoof_SalvageWall',\n",
    "    'VUL_LightRoof_LightWall', \n",
    "    'VUL_LightRoof_SalvageWall', \n",
    "    'VUL_SalvagedRoof_StrongWall',\n",
    "    'VUL_SalvagedRoof_LightWall', \n",
    "    'VUL_SalvagedRoof_SalvageWall',\n",
    "    'VUL_vulnerable_groups', \n",
    "    'VUL_pantawid_pamilya_beneficiary'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6fa3b",
   "metadata": {},
   "source": [
    "### Hyper Parameter optimization  \n",
    "\n",
    "Machine learning models have hyperparameters that you must set in order to customize the model to your dataset. Often the general effects of hyperparameters on a model are known, but how to best set a hyperparameter and combinations of interacting hyperparameters for a given dataset is challenging. There are often general rules of thumb for configuring hyperparameters. A better approach is to objectively search different values for model hyperparameters and choose a subset that results in a model that achieves the best performance on a given dataset. This is called hyperparameter optimization or hyperparameter tuning and is available in the scikit-learn Python machine learning library. [Source](https://machinelearningmastery.com/) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hyperparameters are essentila components for machine learning algorithms, they control behaviour and performance of a machine learning model. For a learning algorithm optimal hyperparameter selection, hyperparameter tuning is esstil first step as it helps to achive best model performance on the data set with a reasonable amount of time.[source](https://www.sciencedirect.com/science/article/pii/S1674862X19300047)\n",
    "\n",
    "To reduce the bias in performance evaluation, model selection should be treated as an integral part of the model fitting procedure, and should be conducted independently in each trial in order to prevent selection bias.[source](https://www.jmlr.org/papers/v11/cawley10a.html)\n",
    "\n",
    "There are different techniques for Hyperparameters, for this research we used neasted K-fold cross validation technique. \n",
    "Nested cross-validation uses inner and outer loops when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. This reduced biased evaluation of model performance as different dataset are used to for hyperparameter tunning and model selection.\n",
    "\n",
    "In our implementation of nested CV the outer loop iterates over typhoon events in our datasets, holiding data for one typhoon for test set and assigning the remaining data as training set. In the inner loop a k-fold CV is applied on the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting input varialbes\n",
    "\n",
    "\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [100, 250],\n",
    "        \"rf__max_depth\": [18, 22],\n",
    "        \"rf__min_samples_split\": [2, 8, 10],\n",
    "        \"rf__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "df_predicted_rf_regr, selected_params_rf_regr = rf_regression_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_perc_dmg',\n",
    "    features=selected_features_rf_regr,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected_Parameters={'rf__max_depth': 22, \n",
    "# 'rf__min_samples_leaf': 3,\n",
    "# 'rf__min_samples_split': 2,\n",
    "# 'rf__n_estimators': 100}\n",
    "\n",
    "#Train score: 0.006772373372310738\n",
    "#Test score: 0.005727847779853535\n",
    "\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\selected_params_rf_regr.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_rf_regr, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_rf_regr.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_rf_regr.to_csv(path)\n",
    "\n",
    "#df_predicted_rf_regr=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf6508",
   "metadata": {},
   "source": [
    "### XGBoost Regression \n",
    "Obtaining the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fb4061a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ATEKLE~1\\AppData\\Local\\Temp/ipykernel_10192/3529970425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Full dataset for feature selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcombined_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DAM_perc_dmg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DAM_perc_dmg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Full dataset for feature selection\n",
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_perc_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "selected_features_xgb_regr, selected_params_xgb_regr_full = xgb_regression_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    min_features_to_select=1,\n",
    "    cv_splits=5,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of selected features XGBoost Regression {len(selected_features_xgb_regr)}\")\n",
    "print(f\"Selected features XGBoost Regression: {selected_features_xgb_regr}\")\n",
    "print(f\"Selected Parameters XGBoost Regression: {selected_params_xgb_regr_full}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffd942",
   "metadata": {},
   "source": [
    "### Obtaining performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the selected features for XGB based on the above cell\n",
    "\n",
    "selected_features_xgb_regr = ['HAZ_rainfall_Total',\n",
    " 'HAZ_v_max',\n",
    " 'HAZ_dis_track_min',\n",
    " 'GEN_landslide_per',\n",
    " 'TOP_mean_elevation_m',\n",
    " 'TOP_mean_ruggedness',\n",
    " 'VUL_Housing_Units',\n",
    " 'VUL_StrongRoof_StrongWall',\n",
    " 'VUL_StrongRoof_LightWall',\n",
    " 'VUL_LightRoof_StrongWall',\n",
    " 'VUL_vulnerable_groups',\n",
    " 'VUL_pantawid_pamilya_beneficiary']\n",
    "\n",
    "selected_params_xgb_regr_full={'estimator__reg_lambda': 0.001,\n",
    " 'estimator__n_estimators': 200,\n",
    " 'estimator__max_depth': 6,\n",
    " 'estimator__learning_rate': 0.1,\n",
    " 'estimator__gamma': 0.1,\n",
    " 'estimator__colsample_bytree': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c83ed",
   "metadata": {},
   "source": [
    "### parameter optimization first based on selected model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"xgb__gamma\": [0.1, 0.5, 2],\n",
    "        \"xgb__max_depth\": [6, 8],\n",
    "        \"xgb__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"xgb__n_estimators\": [100, 200],\n",
    "        \"xgb__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "df_predicted_xgb_regr, selected_params_xgb_regr = xgb_regression_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_perc_dmg',\n",
    "    features=selected_features_xgb_regr,\n",
    "    search_space=xgb_search_space,\n",
    "    cv_splits=5,\n",
    "    objective=\"reg:squarederror\",\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"models\\\\output\\\\02\\\\selected_params_xgb_regr.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_xgb_regr, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_xgb_regr.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_xgb_regr.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb54f47",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "\n",
    "The Baseline model is based on this information (check how to relate result) \n",
    "\n",
    "\n",
    "https://ndrrmc.gov.ph/attachments/article/1509/Component_4_Tropical_Cyclone_Severe_Wind_Technical_Report_-_Final_Draft_by_GA_and_PAGASA.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8929f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_name = \"models\\\\baseline\\\\BASILE_MODEL.xlsx\"\n",
    "\n",
    "path = os.path.join(wor_dir, file_name)\n",
    "VUL_StrongRoof_StrongWall = pd.read_excel(path, sheet_name=\"C1_M\", engine=\"openpyxl\")  #DAM_Strong.Roof.Strong.Wall 80 \n",
    "VUL_StrongRoof_LightWall = pd.read_excel(path, sheet_name=\"CHB_L_W\", engine=\"openpyxl\") #DAM_Strong.Roof.Light.Wall 80 \n",
    "VUL_StrongRoof_SalvageWall = pd.read_excel(path, sheet_name=\"CWS_L_W\", engine=\"openpyxl\") #DAM_Strong.Roof.Salvage.Wall  80\n",
    "VUL_LightRoof_StrongWall = pd.read_excel(path, sheet_name=\"C1_L_S\", engine=\"openpyxl\") #DAM_Light.Roof.Strong.Wall 80\n",
    "VUL_LightRoof_LightWall = pd.read_excel(path, sheet_name=\"W1_L\", engine=\"openpyxl\") #DAM_Light.Roof.Light.Wall 80\n",
    "VUL_SalvagedRoof_LightWall = pd.read_excel(path, sheet_name=\"W3_L\", engine=\"openpyxl\")#DAM_Salvaged.Roof.Light.Wall 50\n",
    "VUL_SalvagedRoof_SalvageWall = pd.read_excel(path, sheet_name=\"N_L\", engine=\"openpyxl\")#DAM_Salvaged.Roof.Salvaged.Wall 80\n",
    "\n",
    "df_damagecurve=[VUL_StrongRoof_StrongWall,\n",
    "VUL_StrongRoof_LightWall ,\n",
    "VUL_StrongRoof_SalvageWall ,\n",
    "VUL_LightRoof_StrongWall ,\n",
    "VUL_LightRoof_LightWall,\n",
    "VUL_SalvagedRoof_LightWall ,\n",
    "VUL_SalvagedRoof_SalvageWall ]\n",
    "\n",
    "vul_list=[\n",
    "    'VUL_StrongRoof_StrongWall',\n",
    "    'VUL_StrongRoof_LightWall' ,\n",
    "    'VUL_StrongRoof_SalvageWall' ,\n",
    "    'VUL_LightRoof_StrongWall' ,\n",
    "    'VUL_LightRoof_LightWall',\n",
    "    'VUL_SalvagedRoof_LightWall' ,\n",
    "    'VUL_SalvagedRoof_SalvageWall' ]\n",
    "\n",
    "df_damagecurve=[df.rename(columns={\"Unnamed: 0\": \"damage_ratio\", \"Unnamed: 1\": \"wind_kmh\"}) for df in df_damagecurve]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree=5)\n",
    "\n",
    "train = combined_input_data\n",
    "input_variable = \"HAZ_v_max\"\n",
    "x_train = 3.6*train[input_variable].values.reshape(-1, 1)\n",
    "y_train = train[\"DAM_perc_dmg\"].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "train_dm={}\n",
    "train_dml=[]\n",
    "\n",
    "for j in range(len(df_damagecurve)):\n",
    "    train_ = df_damagecurve[j]\n",
    "    x_train1 = train_['wind_kmh'].values.reshape(-1, 1)\n",
    "    y_train1 = train_[\"damage_ratio\"].values.reshape(-1, 1)\n",
    "    model = LinearRegression()        \n",
    "    lr_fitted= model.fit(poly_reg.fit_transform(x_train1), y_train1) \n",
    "    y_pred_train =lr_fitted.predict(poly_reg.fit_transform(x_train))\n",
    "\n",
    "\n",
    "    y_pred_train =y_pred_train.flatten()* train[vul_list[j]].values\n",
    "    train_dm[j]=y_pred_train\n",
    "for value in (zip(*list(train_dm.values()))):\n",
    "       train_dml.append(sum(value))\n",
    "        \n",
    "train[\"predicted\"]=train_dml\n",
    "\n",
    "def wind_check(x):\n",
    "    v_max = x[0]  \n",
    "    damage = x[1]\n",
    "    if v_max < 22: ### remove prediction below windspeed below 80km/h \n",
    "        value = 0\n",
    "    else:\n",
    "        value = damage\n",
    "    return value\n",
    "\n",
    "df_predicted_damagecurve = pd.DataFrame({\"typhoon\": train[\"typhoon\"], \"actual\": train[\"DAM_perc_dmg\"].values} )\n",
    "\n",
    "df_predicted_damagecurve[\"predicted\"] = train[[\"predicted\",\"HAZ_v_max\"]].apply(wind_check, axis=\"columns\")\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_damagecurve.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_damagecurve.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "434fd760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typhoon</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.036326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19062</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19072</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19073</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19078</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19079</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          typhoon    actual  predicted\n",
       "0      durian2006  0.036326          0\n",
       "1      durian2006  0.000000          0\n",
       "2      durian2006  0.000000          0\n",
       "4      durian2006  0.000000          0\n",
       "7      durian2006  0.000000          0\n",
       "...           ...       ...        ...\n",
       "19062    noul2015  0.000000          0\n",
       "19072    noul2015  0.000000          0\n",
       "19073    noul2015  0.000000          0\n",
       "19078    noul2015  0.000000          0\n",
       "19079    noul2015  0.000000          0\n",
       "\n",
       "[8556 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted_damagecurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f31b97ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typhoon</th>\n",
       "      <th>HAZ_rainfall_Total</th>\n",
       "      <th>HAZ_rainfall_max_6h</th>\n",
       "      <th>HAZ_rainfall_max_24h</th>\n",
       "      <th>HAZ_v_max</th>\n",
       "      <th>HAZ_dis_track_min</th>\n",
       "      <th>GEN_landslide_per</th>\n",
       "      <th>GEN_stormsurge_per</th>\n",
       "      <th>GEN_Bu_p_inSSA</th>\n",
       "      <th>GEN_Bu_p_LS</th>\n",
       "      <th>...</th>\n",
       "      <th>VUL_LightRoof_StrongWall</th>\n",
       "      <th>VUL_LightRoof_LightWall</th>\n",
       "      <th>VUL_LightRoof_SalvageWall</th>\n",
       "      <th>VUL_SalvagedRoof_StrongWall</th>\n",
       "      <th>VUL_SalvagedRoof_LightWall</th>\n",
       "      <th>VUL_SalvagedRoof_SalvageWall</th>\n",
       "      <th>VUL_vulnerable_groups</th>\n",
       "      <th>VUL_pantawid_pamilya_beneficiary</th>\n",
       "      <th>DAM_perc_dmg</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>182.760714</td>\n",
       "      <td>14.716071</td>\n",
       "      <td>7.381696</td>\n",
       "      <td>55.032241</td>\n",
       "      <td>2.478142</td>\n",
       "      <td>2.64</td>\n",
       "      <td>6.18</td>\n",
       "      <td>6.18</td>\n",
       "      <td>2.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.029515</td>\n",
       "      <td>0.469311</td>\n",
       "      <td>0.036326</td>\n",
       "      <td>0.023732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>27.012500</td>\n",
       "      <td>1.893750</td>\n",
       "      <td>1.070833</td>\n",
       "      <td>23.402905</td>\n",
       "      <td>136.527982</td>\n",
       "      <td>0.78</td>\n",
       "      <td>40.87</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.008676</td>\n",
       "      <td>0.089672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>3.563542</td>\n",
       "      <td>1.050260</td>\n",
       "      <td>8.728380</td>\n",
       "      <td>288.358553</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.259892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>2.408333</td>\n",
       "      <td>0.957639</td>\n",
       "      <td>10.945624</td>\n",
       "      <td>274.953818</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.021318</td>\n",
       "      <td>0.321857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>12.831250</td>\n",
       "      <td>1.054167</td>\n",
       "      <td>0.528125</td>\n",
       "      <td>10.660943</td>\n",
       "      <td>258.194381</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>0.350526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19062</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>12.656896</td>\n",
       "      <td>224.609291</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19072</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>3.287500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>8.136932</td>\n",
       "      <td>277.107823</td>\n",
       "      <td>1.80</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.313084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19073</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>18.015031</td>\n",
       "      <td>145.001814</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.17</td>\n",
       "      <td>5.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>0.372626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19078</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>15.647639</td>\n",
       "      <td>219.542224</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.05</td>\n",
       "      <td>4.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.316341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19079</th>\n",
       "      <td>noul2015</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.102083</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>15.018557</td>\n",
       "      <td>177.362033</td>\n",
       "      <td>1.05</td>\n",
       "      <td>10.84</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.011297</td>\n",
       "      <td>0.190333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8556 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          typhoon  HAZ_rainfall_Total  HAZ_rainfall_max_6h  \\\n",
       "0      durian2006          182.760714            14.716071   \n",
       "1      durian2006           27.012500             1.893750   \n",
       "2      durian2006            0.137500             3.563542   \n",
       "4      durian2006           23.100000             2.408333   \n",
       "7      durian2006           12.831250             1.054167   \n",
       "...           ...                 ...                  ...   \n",
       "19062    noul2015            1.125000             0.108333   \n",
       "19072    noul2015            3.287500             0.325000   \n",
       "19073    noul2015            1.000000             0.058333   \n",
       "19078    noul2015            0.644444             0.062963   \n",
       "19079    noul2015            1.237500             0.102083   \n",
       "\n",
       "       HAZ_rainfall_max_24h  HAZ_v_max  HAZ_dis_track_min  GEN_landslide_per  \\\n",
       "0                  7.381696  55.032241           2.478142               2.64   \n",
       "1                  1.070833  23.402905         136.527982               0.78   \n",
       "2                  1.050260   8.728380         288.358553               0.06   \n",
       "4                  0.957639  10.945624         274.953818               1.52   \n",
       "7                  0.528125  10.660943         258.194381               5.52   \n",
       "...                     ...        ...                ...                ...   \n",
       "19062              0.046875  12.656896         224.609291               0.00   \n",
       "19072              0.095238   8.136932         277.107823               1.80   \n",
       "19073              0.038194  18.015031         145.001814               5.94   \n",
       "19078              0.018056  15.647639         219.542224               4.15   \n",
       "19079              0.046354  15.018557         177.362033               1.05   \n",
       "\n",
       "       GEN_stormsurge_per  GEN_Bu_p_inSSA  GEN_Bu_p_LS  ...  \\\n",
       "0                    6.18            6.18         2.64  ...   \n",
       "1                   40.87           40.80         0.78  ...   \n",
       "2                    0.00            0.00         0.06  ...   \n",
       "4                    1.28            1.28         1.52  ...   \n",
       "7                    0.36            0.36         5.52  ...   \n",
       "...                   ...             ...          ...  ...   \n",
       "19062                0.00            0.00         0.00  ...   \n",
       "19072                6.25            6.25         1.80  ...   \n",
       "19073                1.17            1.17         5.94  ...   \n",
       "19078                3.05            3.05         4.15  ...   \n",
       "19079               10.84           10.80         1.05  ...   \n",
       "\n",
       "       VUL_LightRoof_StrongWall  VUL_LightRoof_LightWall  \\\n",
       "0                      0.000064                 0.000064   \n",
       "1                      0.000097                 0.000097   \n",
       "2                      0.000037                 0.000037   \n",
       "4                      0.000090                 0.000090   \n",
       "7                      0.000118                 0.000118   \n",
       "...                         ...                      ...   \n",
       "19062                  0.000059                 0.000059   \n",
       "19072                  0.000135                 0.000135   \n",
       "19073                  0.000096                 0.000096   \n",
       "19078                  0.000049                 0.000049   \n",
       "19079                  0.000044                 0.000044   \n",
       "\n",
       "       VUL_LightRoof_SalvageWall  VUL_SalvagedRoof_StrongWall  \\\n",
       "0                       0.000064                     0.000064   \n",
       "1                       0.000097                     0.000097   \n",
       "2                       0.000037                     0.000037   \n",
       "4                       0.000090                     0.000090   \n",
       "7                       0.000118                     0.000118   \n",
       "...                          ...                          ...   \n",
       "19062                   0.000059                     0.000059   \n",
       "19072                   0.000135                     0.000135   \n",
       "19073                   0.000096                     0.000096   \n",
       "19078                   0.000049                     0.000049   \n",
       "19079                   0.000044                     0.000044   \n",
       "\n",
       "       VUL_SalvagedRoof_LightWall  VUL_SalvagedRoof_SalvageWall  \\\n",
       "0                        0.000064                      0.000064   \n",
       "1                        0.000097                      0.000097   \n",
       "2                        0.000037                      0.000037   \n",
       "4                        0.000090                      0.000090   \n",
       "7                        0.000118                      0.000118   \n",
       "...                           ...                           ...   \n",
       "19062                    0.000059                      0.000059   \n",
       "19072                    0.000135                      0.000135   \n",
       "19073                    0.000096                      0.000096   \n",
       "19078                    0.000049                      0.000049   \n",
       "19079                    0.000044                      0.000044   \n",
       "\n",
       "       VUL_vulnerable_groups  VUL_pantawid_pamilya_beneficiary  DAM_perc_dmg  \\\n",
       "0                   0.029515                          0.469311      0.036326   \n",
       "1                   0.008676                          0.089672      0.000000   \n",
       "2                   0.033389                          0.259892      0.000000   \n",
       "4                   0.021318                          0.321857      0.000000   \n",
       "7                   0.013870                          0.350526      0.000000   \n",
       "...                      ...                               ...           ...   \n",
       "19062               0.022139                          0.253394      0.000000   \n",
       "19072               0.028278                          0.313084      0.000000   \n",
       "19073               0.032741                          0.372626      0.000000   \n",
       "19078               0.025181                          0.316341      0.000000   \n",
       "19079               0.011297                          0.190333      0.000000   \n",
       "\n",
       "       predicted  \n",
       "0       0.023732  \n",
       "1       0.029787  \n",
       "2       0.122026  \n",
       "4       0.109106  \n",
       "7       0.058244  \n",
       "...          ...  \n",
       "19062   0.102625  \n",
       "19072   0.091696  \n",
       "19073   0.030070  \n",
       "19078   0.051438  \n",
       "19079   0.079625  \n",
       "\n",
       "[8556 rows x 38 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bb6f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline <- function(newdata,threshold) {\n",
    "  newdata <- newdata %>% mutate(X2=WAE_vmax_kph)\n",
    "  path <- './data/BASILE_MODEL.xlsx'\n",
    "  sheets<- excel_sheets(path = path)\n",
    "  for(i in 1:7)\n",
    "  {  if(as.character(sheets[i])==\"C1_M\"){\n",
    "    mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "    #mydf <- mydf %>% mutate(WAE_vmax_kph=X2)\n",
    "    #names(mydf)[2] <- \"WAE_vmax_kph\"\n",
    "    cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "    \n",
    "    y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "    newdata <- newdata  %>% dplyr::mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "    newdata <- newdata  %>% dplyr::mutate(DAM_Strong.Roof.Strong.Wall=pre_damage*MAT_Strong.Roof.Strong.Wall*100/TP)\n",
    "  }\n",
    "    if(as.character(sheets[i])==\"CHB_L_W\"){     ### CHB_L_W    x<-seq(100,350,10)\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "      \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "     # newdata <- newdata %>% mutate(pre_damage=ifelse ((newdata$WAE_vmax_kph <80 | y_predicted < 0), 0, y_predicted))\n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Strong.Roof.Light.Wall=pre_damage*MAT_Strong.Roof.Light.Wall*100/TP)\n",
    "    }\n",
    "    if(as.character(sheets[i])==\"CWS_L_W\"){ ### CWS_L_W    x<-seq(100,350,10)\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "      \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "     # newdata <- newdata %>% mutate(pre_damage=ifelse (newdata$WAE_vmax_kph >200, 1, y_predicted)) %>%        mutate(pre_damage=ifelse ((newdata$WAE_vmax_kph <80 | y_predicted < 0), 0, y_predicted))\n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Strong.Roof.Salvage.Wall=pre_damage*MAT_Strong.Roof.Salvage.Wall*100/TP)\n",
    "      \n",
    "    }\n",
    "    if(as.character(sheets[i])==\"C1_L_S\"){\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "      #newdata <- newdata %>% mutate(pre_damage=ifelse ((newdata$WAE_vmax_kph <80 | y_predicted < 0), 0, y_predicted))\n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Light.Roof.Strong.Wall=pre_damage*MAT_Light.Roof.Strong.Wall*100/TP)\n",
    "      \n",
    "    }\n",
    "    if(as.character(sheets[i])==\"W1_L\"){\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "     # newdata <- newdata %>%  mutate(pre_damage=ifelse ((newdata$WAE_vmax_kph <80 | y_predicted < 0), 0, y_predicted))\n",
    "      \n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Light.Roof.Light.Wall=pre_damage*MAT_Light.Roof.Light.Wall*100/TP)\n",
    "    }\n",
    "    if(as.character(sheets[i])==\"W3_L\"){     #####W3-L    x<-seq(50,180,10)\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      \n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata) \n",
    "      \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >50) & (y_predicted > threshold)), 1, 0))\n",
    "     # newdata <- newdata %>% mutate(pre_damage=ifelse (newdata$WAE_vmax_kph >200, 1, y_predicted)) %>%         mutate(pre_damage=ifelse ((newdata$WAE_vmax_kph <80 | y_predicted < 0), 0, y_predicted))\n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Salvaged.Roof.Light.Wall=pre_damage*MAT_Salvaged.Roof.Light.Wall*100/TP)\n",
    "      \n",
    "    }\n",
    "    if(as.character(sheets[i])==\"N_L\"){    ########### N-L   #    x<-seq(80,200,10)\n",
    "      \n",
    "      cvfit_2 <- lm(X1 ~ poly(WAE_vmax_kph, degree = 5, raw = TRUE), data=mydf)\n",
    "      mydf <- openxlsx::read.xlsx(path, sheet = i, startRow = 1, colNames = FALSE) %>% mutate(WAE_vmax_kph=X2) %>% select(X1,WAE_vmax_kph)\n",
    "      y_predicted <- predict(cvfit_2, newdata = newdata)\n",
    "      \n",
    "      newdata <- newdata  %>% mutate(pre_damage=ifelse (((newdata$WAE_vmax_kph >80) & (y_predicted > threshold)), 1, 0))\n",
    "      #newdata <- newdata %>% mutate(pre_damage=ifelse (newdata$WAE_vmax_kph >200, 1, y_predicted)) %>%         mutate(pre_damage=ifelse (newdata$WAE_vmax_kph <80, 0, y_predicted))\n",
    "      newdata <- newdata  %>% dplyr::mutate(DAM_Salvaged.Roof.Salvaged.Wall=pre_damage*MAT_Salvaged.Roof.Salvage.Wall*100/TP)\n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "  return(newdata)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "baseline_prediction<-baseline(newdata=df,threshold=0.55) %>%\n",
    "  dplyr::mutate(Damage_Baseline=(DAM_Strong.Roof.Light.Wall+\n",
    "                                DAM_Light.Roof.Light.Wall+\n",
    "                                DAM_Strong.Roof.Salvage.Wall+\n",
    "                                DAM_Salvaged.Roof.Light.Wall+\n",
    "                                DAM_Strong.Roof.Strong.Wall+\n",
    "                                DAM_Light.Roof.Strong.Wall+\n",
    "                                DAM_Salvaged.Roof.Salvaged.Wall))%>%\n",
    "  dplyr::select(Damage_Baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4856aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the average\n",
    "df_predicted_mean = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_perc_dmg\"]\n",
    "    y_test = test[\"DAM_perc_dmg\"]\n",
    "\n",
    "    y_test_pred = [np.mean(y_train)] * len(y_test)\n",
    "\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_test_pred}\n",
    "    )\n",
    "\n",
    "    df_predicted_mean = pd.concat([df_predicted_mean, df_predicted_temp])\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_mean.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_mean.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea26a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simle Linear Regression with Wind Speed\n",
    "input_variable = \"HAZ_v_max\"\n",
    "df_predicted_lr = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    x_train = train[input_variable].values.reshape(-1, 1)\n",
    "    y_train = train[\"DAM_perc_dmg\"].values.reshape(-1, 1)\n",
    "\n",
    "    x_test = test[input_variable].values.reshape(-1, 1)\n",
    "    y_test = test[\"DAM_perc_dmg\"]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    lr_fitted = model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_train = lr_fitted.predict(x_train)\n",
    "    y_pred_test = lr_fitted.predict(x_test)\n",
    "    y_pred_test = y_pred_test.tolist()\n",
    "    y_pred_test = [val for sublist in y_pred_test for val in sublist]\n",
    "\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_lr = pd.concat([df_predicted_lr, df_predicted_temp])\n",
    "    \n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_lr.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_lr.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
